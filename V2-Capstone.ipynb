{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"V2-Capstone.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"5p3SN4pY4_kR","colab_type":"code","outputId":"25a174bd-f051-48e9-c921-bb2539a7285f","executionInfo":{"status":"ok","timestamp":1562600163873,"user_tz":420,"elapsed":5049,"user":{"displayName":"Sahanaa Mohan Kumar","photoUrl":"","userId":"04718077978566608325"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from collections import OrderedDict\n","\n","from sklearn.datasets import load_files\n","from sklearn.model_selection import train_test_split\n","\n","#Importing Keras Libraries\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D\n","from keras.layers.core import Dense, Dropout\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing import image\n","\n","#Import Video Processing Libraries\n","from skvideo.io import FFmpegReader, ffprobe\n","from skvideo.utils import rgb2gray\n","from PIL import Image\n","from tqdm import tqdm_notebook as tqdm\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8AnFTwDH5LEI","colab_type":"code","outputId":"50effb76-77ce-4e31-ca7d-f2723e7807bf","executionInfo":{"status":"ok","timestamp":1562600096730,"user_tz":420,"elapsed":6252,"user":{"displayName":"Sahanaa Mohan Kumar","photoUrl":"","userId":"04718077978566608325"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["pip install scikit-video"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting scikit-video\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.16.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.3.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->scikit-video) (0.46)\n","Installing collected packages: scikit-video\n","Successfully installed scikit-video-1.1.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-r9CDz14_kb","colab_type":"code","colab":{}},"source":["# Image Functions\n","# 1. Get Total Frame Count in a Video\n","def getFrameCount(filepath):\n","    cap = FFmpegReader(filename=filepath)\n","    framecnt = cap.getShape()[0]\n","    cap.close()\n","    return framecnt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iluB91Qx4_kf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":336},"outputId":"c18d3ec3-97c1-487f-9791-114955cfcb6e","executionInfo":{"status":"error","timestamp":1562600748631,"user_tz":420,"elapsed":875,"user":{"displayName":"Sahanaa Mohan Kumar","photoUrl":"","userId":"04718077978566608325"}}},"source":["raw_data = load_files(r'C/conFusion_3', shuffle=False)\n","files = raw_data['filenames']\n","targets = raw_data['target']"],"execution_count":15,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e68a4dff170d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C/conFusion_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filenames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/datasets/base.py\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[1;32m    164\u001b[0m                if isdir(join(container_path, f))]\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C/conFusion_3'"]}]},{"cell_type":"code","metadata":{"id":"tjMaVuGv4_ki","colab_type":"code","outputId":"0fe5f5fa-88c0-4b07-86b5-f7bf17a63a71","colab":{}},"source":["frms = []\n","for f in files:\n","    frms.append(getFrameCount(f))\n","    \n","unique_elements, counts_elements = np.unique(frms, return_counts=True)\n","print(\"Frequency of unique values of the said array:\")\n","print(np.asarray((unique_elements, counts_elements)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Frequency of unique values of the said array:\n","[[ 22  23  24  30  31  33  34  36  37  38  39  40  41  42  43  44  45  46\n","   47  48  49  50  51  52  53  54  55  57  59  61  63  64  65  66  67  75\n","   77  84 108 109]\n"," [  2   3   1   2   2   2   2   3   4   5   4   3   7   4   6   3  11   4\n","   11  10   8   3   3   4   2   1   3   1   1   3   1   2   1   1   1   2\n","    1   1   1   1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-icX6F3v4_kp","colab_type":"code","colab":{}},"source":["files_list = []\n","for i in range(len(targets)):\n","    tempdic = OrderedDict()            \n","    tempdic[\"FilePath\"] = files[i]\n","    tempdic[\"ActionType\"] = targets[i]\n","    tempdic[\"FrameCount\"] = getFrameCount(files[i])\n","    if tempdic[\"FrameCount\"] >= 40:\n","        files_list.append(tempdic)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWpQfKTX4_kt","colab_type":"code","outputId":"552d8f7b-7397-436f-f254-6a57077612ab","colab":{}},"source":["dataset = pd.DataFrame(files_list)\n","dataset.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FilePath</th>\n","      <th>ActionType</th>\n","      <th>FrameCount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>D:/AIML/DATASET/CapStone/MP4_1\\hit\\50_FIRST_DA...</td>\n","      <td>0</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>D:/AIML/DATASET/CapStone/MP4_1\\hit\\50_FIRST_DA...</td>\n","      <td>0</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>D:/AIML/DATASET/CapStone/MP4_1\\hit\\AmericanGan...</td>\n","      <td>0</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>D:/AIML/DATASET/CapStone/MP4_1\\hit\\Collins_get...</td>\n","      <td>0</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>D:/AIML/DATASET/CapStone/MP4_1\\hit\\Collins_get...</td>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            FilePath  ActionType  FrameCount\n","0  D:/AIML/DATASET/CapStone/MP4_1\\hit\\50_FIRST_DA...           0          44\n","1  D:/AIML/DATASET/CapStone/MP4_1\\hit\\50_FIRST_DA...           0          48\n","2  D:/AIML/DATASET/CapStone/MP4_1\\hit\\AmericanGan...           0          50\n","3  D:/AIML/DATASET/CapStone/MP4_1\\hit\\Collins_get...           0          41\n","4  D:/AIML/DATASET/CapStone/MP4_1\\hit\\Collins_get...           0          43"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"dDkadkx84_ky","colab_type":"code","colab":{}},"source":["filepaths = dataset.FilePath.tolist()\n","actiontypes = dataset.ActionType.tolist()\n","\n","#filepaths = files\n","#actiontypes = targets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pitkib84_k2","colab_type":"code","colab":{}},"source":["MAX_FRAMES = 20\n","REQ_FPS = 10\n","IS_VALID = range(REQ_FPS)\n","No_OF_CLASS = len(np.unique(actiontypes))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FX4ViYzP4_k5","colab_type":"code","colab":{}},"source":["def getVideo(filepath):\n","    cap = FFmpegReader(filename=filepath)\n","    #print('shape:',cap.getShape())\n","    list_of_frames = []\n","    fps = int(cap.inputfps)\n","    \n","    for index, frame in enumerate(cap.nextFrame()):\n","        capture_frame = True\n","        capture_frame = (index % fps) in IS_VALID\n","        if capture_frame:\n","            #print(index)\n","            temp_image = image.array_to_img(frame)\n","            frame = image.img_to_array(temp_image.resize((128,128),Image.ANTIALIAS)).astype('uint8')\n","            list_of_frames.append(frame)\n","\n","    temp_video = np.stack(list_of_frames)\n","    temp_video = rgb2gray(temp_video)\n","    #print(np.size(list_of_frames))\n","    #print('Total Frames:',temp_video.shape)\n","    cap.close()\n","    \n","    total_frames = temp_video.shape[0]\n","    if MAX_FRAMES <= total_frames:\n","        front = ((total_frames - MAX_FRAMES) // 2) + 1\n","        if front == 1:\n","            front = 0\n","        temp_video = temp_video[front:(front + MAX_FRAMES)]\n","    #print('Total Frames:',temp_video.shape)\n","    return np.expand_dims(temp_video, axis=0)\n","\n","def getVideoTensor(path, normalize_pixels):\n","    list_of_videos = [getVideo(p) for p in tqdm(path)]\n","    tensor = np.vstack(list_of_videos)\n","    base = normalize_pixels[0]\n","    r = normalize_pixels[1] - base\n","    min_ = np.min(tensor, axis=(1, 2, 3), keepdims=True)\n","    max_ = np.max(tensor, axis=(1, 2, 3), keepdims=True)\n","    \n","    return ((tensor.astype('float32') - min_) / (max_ - min_)) * r + base"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNSqMMoy4_k9","colab_type":"code","colab":{}},"source":["train_files, test_files, train_targets, test_targets = train_test_split(filepaths, actiontypes, test_size=1/3, random_state=100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-GcoMq34_lC","colab_type":"code","outputId":"82d3654c-56f6-4eb6-df87-92e53277de71","colab":{}},"source":["print('Total number of videos:', len(filepaths))\n","print('\\nNumber of videos in training data:', len(train_files))\n","print('Number of videos in test data:', len(test_files))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total number of videos: 342\n","\n","Number of videos in training data: 228\n","Number of videos in test data: 114\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PnEoHTcZ4_lI","colab_type":"code","colab":{}},"source":["len_train = len(train_files)\n","val_len = len_train - int(len_train*0.25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IuUV6Nf4_lL","colab_type":"code","colab":{}},"source":["# Taking ~25% of the training data for validation\n","valid_files = train_files[val_len:]\n","valid_targets = train_targets[val_len:]\n","\n","# Remaining data will be used for training the model\n","train_files = train_files[:val_len]\n","train_targets = train_targets[:val_len]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5S639w484_lP","colab_type":"code","outputId":"31162534-0da3-42d3-848a-00b1842992c3","colab":{}},"source":["print('Number of videos in training data:', len(train_files))\n","print('Number of videos in validation data:', len(valid_files))\n","print('Number of videos in test data:', len(test_files))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of videos in training data: 171\n","Number of videos in validation data: 57\n","Number of videos in test data: 114\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xOv85hUl4_lT","colab_type":"code","outputId":"100b9d9e-a441-4026-8577-d31c87fe990b","colab":{}},"source":["X_train = getVideoTensor(train_files, (-1,1))\n","y_train = to_categorical(train_targets, num_classes=No_OF_CLASS)\n","print('Shape of training data:', X_train.shape)\n","print('Shape of training labels:', y_train.shape)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1230db77c1464254b73cdea39a3084ab","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=171), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Shape of training data: (171, 20, 128, 128, 1)\n","Shape of training labels: (171, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"shxrjw4N4_lY","colab_type":"code","outputId":"21530746-d146-4d98-b8cd-9023c717d31d","colab":{}},"source":["# Reading validation videos and one-hot encoding the validation labels\n","X_valid = getVideoTensor(valid_files, (-1,1))\n","y_valid = to_categorical(valid_targets, num_classes=No_OF_CLASS)\n","print('Shape of validation data:', X_valid.shape)\n","print('Shape of validation labels:', y_valid.shape)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d12bf7c17394d63a810b1f8c57f3dfb","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Shape of validation data: (57, 20, 128, 128, 1)\n","Shape of validation labels: (57, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bSY2xHO14_ld","colab_type":"code","outputId":"0c2652f5-e05d-4029-cd3f-330242e0996b","colab":{}},"source":["# Reading testing videos and one-hot encoding the testing labels\n","X_test = getVideoTensor(test_files, (-1,1))\n","y_test = to_categorical(test_targets, num_classes=No_OF_CLASS)\n","print('Shape of testing data:', X_test.shape)\n","print('Shape of testing labels:', y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db089e6c020c4c3c86f1ac33e258994c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=114), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Shape of testing data: (114, 20, 128, 128, 1)\n","Shape of testing labels: (114, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0XQTnsej4_ll","colab_type":"code","outputId":"6b147b75-202a-4f58-960d-3cf4cbf73a36","colab":{}},"source":["# Using the Sequential Model\n","model = Sequential()\n","\n","# Adding Alternate convolutional and pooling layers\n","model.add(Conv3D(filters=16, kernel_size=(5, 3, 3), strides=(1, 1, 1), padding='same', activation='relu', \n","                 input_shape=X_train.shape[1:]))\n","model.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))\n","\n","model.add(Conv3D(filters=64, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='valid', activation='relu'))\n","model.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))\n","\n","model.add(Conv3D(filters=256, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='valid', activation='relu'))\n","model.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))\n","\n","model.add(Conv3D(filters=1024, kernel_size=(2, 3, 3), strides=(1, 1, 1), padding='valid', activation='relu'))\n","model.add(MaxPooling3D(pool_size=2, strides=(2, 2, 2), padding='same'))\n","\n","# A global average pooling layer to get a 1-d vector\n","# The vector will have a depth (same as number of elements in the vector) of 1024\n","model.add(GlobalAveragePooling3D())\n","\n","# Hidden layer\n","model.add(Dense(32, activation='relu'))\n","\n","# Dropout Layer\n","model.add(Dropout(0.5))\n","\n","# Output layer\n","model.add(Dense(No_OF_CLASS, activation='softmax'))\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_1 (Conv3D)            (None, 20, 128, 128, 16)  736       \n","_________________________________________________________________\n","max_pooling3d_1 (MaxPooling3 (None, 10, 64, 64, 16)    0         \n","_________________________________________________________________\n","conv3d_2 (Conv3D)            (None, 9, 62, 62, 64)     18496     \n","_________________________________________________________________\n","max_pooling3d_2 (MaxPooling3 (None, 5, 31, 31, 64)     0         \n","_________________________________________________________________\n","conv3d_3 (Conv3D)            (None, 4, 29, 29, 256)    295168    \n","_________________________________________________________________\n","max_pooling3d_3 (MaxPooling3 (None, 2, 15, 15, 256)    0         \n","_________________________________________________________________\n","conv3d_4 (Conv3D)            (None, 1, 13, 13, 1024)   4719616   \n","_________________________________________________________________\n","max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 1024)     0         \n","_________________________________________________________________\n","global_average_pooling3d_1 ( (None, 1024)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                32800     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 32)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 132       \n","=================================================================\n","Total params: 5,066,948\n","Trainable params: 5,066,948\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B8A0LtjI4_ls","colab_type":"code","outputId":"18b8894f-fcb7-4b32-aeb0-2aed8adbbf90","colab":{}},"source":["# Compiling the model\n","model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n","\n","# Saving the model that performed the best on the validation set\n","checkpoint = ModelCheckpoint(filepath='Model_x.weights.best.hdf5', save_best_only=True, verbose=1)\n","\n","# Training the model for 40 epochs\n","history = model.fit(X_train, y_train, batch_size=16, epochs=3, \n","                    validation_data=(X_valid, y_valid), verbose=2, callbacks=[checkpoint])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 171 samples, validate on 57 samples\n","Epoch 1/3\n"," - 622s - loss: 6.7794 - acc: 0.2690 - val_loss: 10.7454 - val_acc: 0.3333\n","\n","Epoch 00001: val_loss improved from inf to 10.74540, saving model to Model_x.weights.best.hdf5\n","Epoch 2/3\n"," - 624s - loss: 6.2010 - acc: 0.3567 - val_loss: 10.7454 - val_acc: 0.3333\n","\n","Epoch 00002: val_loss did not improve from 10.74540\n","Epoch 3/3\n"," - 617s - loss: 5.4604 - acc: 0.3275 - val_loss: 10.7454 - val_acc: 0.3333\n","\n","Epoch 00003: val_loss did not improve from 10.74540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lu2J0JH-4_ly","colab_type":"code","outputId":"af4863e0-b998-4ddb-b05f-75fad3692de1","colab":{}},"source":["# Loading the model that performed the best on the validation set\n","model.load_weights('Model_x.weights.best.hdf5')\n","\n","# Testing the model on the Test data\n","(loss, accuracy) = model.evaluate(X_test, y_test, batch_size=16, verbose=0)\n","\n","print('Accuracy on test data: {:.2f}%'.format(accuracy * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy on test data: 23.68%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PPdxIkW_4_l4","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":40},"outputId":"37c5cd99-e5db-4b3e-daea-cf579cf2fac0","executionInfo":{"status":"ok","timestamp":1562601439532,"user_tz":420,"elapsed":33062,"user":{"displayName":"Sahanaa Mohan Kumar","photoUrl":"","userId":"04718077978566608325"}}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-bb45e91d-8832-40f9-b261-354082ac6c7d\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-bb45e91d-8832-40f9-b261-354082ac6c7d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]}]}