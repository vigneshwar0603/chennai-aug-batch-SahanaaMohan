{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFfDTfhlaEI_"
   },
   "source": [
    "# Transfer Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNwbqCFRaEJC"
   },
   "source": [
    "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUB1uDW_8XIy"
   },
   "source": [
    "## 1. Import necessary libraries for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rsj4t5HTaEJE"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXrn3heBaEJa"
   },
   "source": [
    "## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjDuiK6ztgOK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTrainX=trainX[trainY<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bTrainX=trainX[trainY>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTestX=testX[testY<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bTestX=testX[testY>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set 1 shape (30596, 28, 28)\n",
      "Data set 2 shape (29404, 28, 28)\n",
      "Data set 3 shape (5139, 28, 28)\n",
      "Data set 4 shape (4861, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Data set 1 shape',aTrainX.shape)\n",
    "print('Data set 2 shape',bTrainX.shape)\n",
    "print('Data set 3 shape',aTestX.shape)\n",
    "print('Data set 4 shape',bTestX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qU14lYL9A5g"
   },
   "source": [
    "## 3. Print x_train, y_train, x_test and y_test for both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9OrszhJ0SgJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set 1  [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "Data set 2  [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "Data set 3  [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "Data set 4  [[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print('Data set 1 ',aTrainX)\n",
    "print('Data set 2 ',bTrainX)\n",
    "print('Data set 3 ',aTestX)\n",
    "print('Data set 4 ',bTestX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB9BPFzr9oDF"
   },
   "source": [
    "## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n",
    "## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlQRPfFzaEJx"
   },
   "outputs": [],
   "source": [
    "aTrainX = aTrainX.reshape(aTrainX.shape[0], 28, 28,1)\n",
    "aTestX = aTestX.reshape(aTestX.shape[0], 28, 28,1)\n",
    "aTrainX = aTrainX.astype('float32')\n",
    "aTestX = aTestX.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLQr-b3F-hw8"
   },
   "source": [
    "## 5. Normalize x_train and x_test by dividing it by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlEZIAG5-g2I"
   },
   "outputs": [],
   "source": [
    "aTrainX /= 255\n",
    "aTestX /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pytVBaw4-vMi"
   },
   "source": [
    "## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY=trainY[trainY<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY=testY[testY<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainYEncoded = tf.keras.utils.to_categorical(trainY)\n",
    "testYEncoded = tf.keras.utils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elPkI44g_C2b"
   },
   "source": [
    "## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set 1 shape (30596, 28, 28, 1)\n",
      "Data set 2 shape (30596, 5)\n",
      "Data set 3 shape (5139, 28, 28, 1)\n",
      "Data set 4 shape (5139, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Data set 1 shape',aTrainX.shape)\n",
    "print('Data set 2 shape',trainYEncoded.shape)\n",
    "print('Data set 3 shape',aTestX.shape)\n",
    "print('Data set 4 shape',testYEncoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MU09mm9F89gO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/10\n",
      "30596/30596 [==============================] - 16s 538us/step - loss: 0.4462 - acc: 0.8523 - val_loss: 0.1625 - val_acc: 0.9510\n",
      "Epoch 2/10\n",
      "30596/30596 [==============================] - 10s 340us/step - loss: 0.2319 - acc: 0.9287 - val_loss: 0.1214 - val_acc: 0.9615\n",
      "Epoch 3/10\n",
      "30596/30596 [==============================] - 10s 343us/step - loss: 0.1929 - acc: 0.9410 - val_loss: 0.1010 - val_acc: 0.9681\n",
      "Epoch 4/10\n",
      "30596/30596 [==============================] - 10s 329us/step - loss: 0.1705 - acc: 0.9470 - val_loss: 0.0913 - val_acc: 0.9714\n",
      "Epoch 5/10\n",
      "30596/30596 [==============================] - 10s 329us/step - loss: 0.1569 - acc: 0.9510 - val_loss: 0.0903 - val_acc: 0.9714\n",
      "Epoch 6/10\n",
      "30596/30596 [==============================] - 11s 345us/step - loss: 0.1439 - acc: 0.9555 - val_loss: 0.0861 - val_acc: 0.9722\n",
      "Epoch 7/10\n",
      "30596/30596 [==============================] - 10s 328us/step - loss: 0.1360 - acc: 0.9582 - val_loss: 0.0738 - val_acc: 0.9753\n",
      "Epoch 8/10\n",
      "30596/30596 [==============================] - 10s 336us/step - loss: 0.1356 - acc: 0.9574 - val_loss: 0.0726 - val_acc: 0.9768\n",
      "Epoch 9/10\n",
      "30596/30596 [==============================] - 11s 345us/step - loss: 0.1218 - acc: 0.9623 - val_loss: 0.0701 - val_acc: 0.9780\n",
      "Epoch 10/10\n",
      "30596/30596 [==============================] - 11s 343us/step - loss: 0.1204 - acc: 0.9625 - val_loss: 0.0651 - val_acc: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f9ad083550>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    # Define Model\n",
    "model3 = Sequential()\n",
    "\n",
    "    # 1st Conv Layer\n",
    "model3.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "    # 2nd Conv Layer\n",
    "model3.add(Convolution2D(32, 3, 3))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "    # Max Pooling\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Dropout\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128))\n",
    "model3.add(Activation('relu'))\n",
    "    \n",
    "    # More Dropout\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "    # Prediction Layer\n",
    "model3.add(Dense(5))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "    # Loss and Optimizer\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Store Training Results\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=7, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]\n",
    "\n",
    "    # Train the model\n",
    "model3.fit(aTrainX, trainYEncoded, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "              validation_data=(aTestX, testYEncoded), callbacks=callback_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJQaycRO_3Au"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOZeRbK7t9AT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "my1P09bxAv8H"
   },
   "source": [
    "## 9. Print the training and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf7F8Gdutbf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z78o3WIjaEJ3"
   },
   "source": [
    "## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brN7VZHFaEJ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4opnW7o0BJ8P"
   },
   "source": [
    "## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCFcYHTm6-cE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DITyAt3t7Tto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoDozqghCJZ4"
   },
   "source": [
    "## 12. Print the accuracy for classification of digits 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fCxgb5s49Cj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRWizZIpCUKg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "# read tweets.csv into a DataFrame\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv('tweets.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(axis = 0, how ='any') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of                                              tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "7     #SXSW is just starting, #CTIA is around the co...   \n",
       "8     Beautifully smart and simple idea RT @madebyma...   \n",
       "9     Counting down the days to #sxsw plus strong Ca...   \n",
       "10    Excited to meet the @samsungmobileus at #sxsw ...   \n",
       "11    Find &amp; Start Impromptu Parties at #SXSW Wi...   \n",
       "12    Foursquare ups the game, just in time for #SXS...   \n",
       "13    Gotta love this #SXSW Google Calendar featurin...   \n",
       "14    Great #sxsw ipad app from @madebymany: http://...   \n",
       "15    haha, awesomely rad iPad app by @madebymany ht...   \n",
       "17    I just noticed DST is coming this weekend. How...   \n",
       "18    Just added my #SXSW flights to @planely. Match...   \n",
       "19    Must have #SXSW app! RT @malbonster: Lovely re...   \n",
       "20    Need to buy an iPad2 while I'm in Austin at #s...   \n",
       "21    Oh. My. God. The #SXSW app for iPad is pure, u...   \n",
       "22    Okay, this is really it: yay new @Foursquare f...   \n",
       "23    Photo: Just installed the #SXSW iPhone app, wh...   \n",
       "24    Really enjoying the changes in Gowalla 3.0 for...   \n",
       "25    RT @LaurieShook: I'm looking forward to the #S...   \n",
       "26    RT haha, awesomely rad iPad app by @madebymany...   \n",
       "27    someone started an #austin @PartnerHub group i...   \n",
       "28    The new #4sq3 looks like it is going to rock. ...   \n",
       "29    They were right, the @gowalla 3 app on #androi...   \n",
       "30    Very smart from @madebymany #hollergram iPad a...   \n",
       "31    You must have this app for your iPad if you ar...   \n",
       "36    The best!  RT @mention Ha! First in line for #...   \n",
       "...                                                 ...   \n",
       "9008  I'm pretty sure the panelist that thinks &quot...   \n",
       "9009  Very happy that Discovr has been named as one ...   \n",
       "9012  Apparently there is an iPad and iPhone app to ...   \n",
       "9013  On the way to #sxsw, see you all tonight! hope...   \n",
       "9017  Stopped by Tron Legacy Lounge at  #SXSW. Audio...   \n",
       "9018  Second day using my Apple iPad2 at #SXSW and I...   \n",
       "9022  By the way, I love that y'all are so down to d...   \n",
       "9025  Absolutely!  RT @mention Timely good schtuff f...   \n",
       "9027  Good job y'all!  RT @mention Yes! Gowalla wins...   \n",
       "9029  [TOP STORY] At #SXSW, Apple schools the market...   \n",
       "9033  @mention yep! I can't believe they set up a po...   \n",
       "9035  @mention Yes, I picked up the ipad 2 at #SXSW....   \n",
       "9036  @mention Yes, that's why I favorited it! I wan...   \n",
       "9044  Look everyone! Zomg @mention got an iPad 2 on ...   \n",
       "9045  @mention you are my favorite-- thanks for comi...   \n",
       "9048  @mention You bet man! Kindle and Apple for sur...   \n",
       "9051  @mention You can get an iPad 1 for under $350 ...   \n",
       "9060  @mention you might also appreciate new iPhone ...   \n",
       "9061  @mention You realize I'm still padless? I just...   \n",
       "9063  @mention You should get the iPad 2  to save yo...   \n",
       "9064  @mention you should see the line here at #SXSW...   \n",
       "9066  How much you want to bet Apple is disproportio...   \n",
       "9070  You know you've made it to #sxsw when you see ...   \n",
       "9071  what are your essentials for #SxSW?  Mine? poc...   \n",
       "9072  @mention your iPhone 4 cases are Rad and Ready...   \n",
       "9077  @mention your PR guy just convinced me to swit...   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   \n",
       "9080  Diller says Google TV &quot;might be run over ...   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "\n",
       "      emotion_in_tweet_is_directed_at  \\\n",
       "0                              iPhone   \n",
       "1                  iPad or iPhone App   \n",
       "2                                iPad   \n",
       "3                  iPad or iPhone App   \n",
       "4                              Google   \n",
       "7                             Android   \n",
       "8                  iPad or iPhone App   \n",
       "9                               Apple   \n",
       "10                            Android   \n",
       "11                        Android App   \n",
       "12                        Android App   \n",
       "13    Other Google product or service   \n",
       "14                 iPad or iPhone App   \n",
       "15                 iPad or iPhone App   \n",
       "17                             iPhone   \n",
       "18                 iPad or iPhone App   \n",
       "19                 iPad or iPhone App   \n",
       "20                               iPad   \n",
       "21                 iPad or iPhone App   \n",
       "22                        Android App   \n",
       "23                 iPad or iPhone App   \n",
       "24                        Android App   \n",
       "25                               iPad   \n",
       "26                 iPad or iPhone App   \n",
       "27    Other Google product or service   \n",
       "28                 iPad or iPhone App   \n",
       "29                        Android App   \n",
       "30                 iPad or iPhone App   \n",
       "31                 iPad or iPhone App   \n",
       "36                               iPad   \n",
       "...                               ...   \n",
       "9008                            Apple   \n",
       "9009               iPad or iPhone App   \n",
       "9012               iPad or iPhone App   \n",
       "9013                             iPad   \n",
       "9017                             iPad   \n",
       "9018                             iPad   \n",
       "9022                            Apple   \n",
       "9025  Other Google product or service   \n",
       "9027                      Android App   \n",
       "9029                            Apple   \n",
       "9033                            Apple   \n",
       "9035                             iPad   \n",
       "9036                             iPad   \n",
       "9044                             iPad   \n",
       "9045                           iPhone   \n",
       "9048                            Apple   \n",
       "9051                             iPad   \n",
       "9060               iPad or iPhone App   \n",
       "9061                             iPad   \n",
       "9063                             iPad   \n",
       "9064                            Apple   \n",
       "9066                            Apple   \n",
       "9070                             iPad   \n",
       "9071                             iPad   \n",
       "9072                           iPhone   \n",
       "9077                           iPhone   \n",
       "9079                             iPad   \n",
       "9080  Other Google product or service   \n",
       "9085               iPad or iPhone App   \n",
       "9088                             iPad   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "7                                      Positive emotion  \n",
       "8                                      Positive emotion  \n",
       "9                                      Positive emotion  \n",
       "10                                     Positive emotion  \n",
       "11                                     Positive emotion  \n",
       "12                                     Positive emotion  \n",
       "13                                     Positive emotion  \n",
       "14                                     Positive emotion  \n",
       "15                                     Positive emotion  \n",
       "17                                     Negative emotion  \n",
       "18                                     Positive emotion  \n",
       "19                                     Positive emotion  \n",
       "20                                     Positive emotion  \n",
       "21                                     Positive emotion  \n",
       "22                                     Positive emotion  \n",
       "23                                     Positive emotion  \n",
       "24                                     Positive emotion  \n",
       "25                                     Positive emotion  \n",
       "26                                     Positive emotion  \n",
       "27                                     Positive emotion  \n",
       "28                                     Positive emotion  \n",
       "29                                     Positive emotion  \n",
       "30                                     Positive emotion  \n",
       "31                                     Positive emotion  \n",
       "36                                     Positive emotion  \n",
       "...                                                 ...  \n",
       "9008                                   Negative emotion  \n",
       "9009                                   Positive emotion  \n",
       "9012                                   Positive emotion  \n",
       "9013                                   Positive emotion  \n",
       "9017                                   Positive emotion  \n",
       "9018                                   Positive emotion  \n",
       "9022                                   Positive emotion  \n",
       "9025                                   Positive emotion  \n",
       "9027                                   Positive emotion  \n",
       "9029                                   Positive emotion  \n",
       "9033                                   Positive emotion  \n",
       "9035                                   Positive emotion  \n",
       "9036                                   Positive emotion  \n",
       "9044                                   Positive emotion  \n",
       "9045                                   Positive emotion  \n",
       "9048                                   Positive emotion  \n",
       "9051                 No emotion toward brand or product  \n",
       "9060                                   Positive emotion  \n",
       "9061                                   Positive emotion  \n",
       "9063                                   Positive emotion  \n",
       "9064                                   Positive emotion  \n",
       "9066                                       I can't tell  \n",
       "9070                                   Positive emotion  \n",
       "9071                                   Positive emotion  \n",
       "9072                                   Positive emotion  \n",
       "9077                                   Positive emotion  \n",
       "9079                                   Positive emotion  \n",
       "9080                                   Negative emotion  \n",
       "9085                                   Positive emotion  \n",
       "9088                                   Positive emotion  \n",
       "\n",
       "[3291 rows x 3 columns]>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return text.decode('ascii')\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [],
   "source": [
    "tweets['text'] = [preprocess(text) for text in tweets.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kX-WoJDH0NV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW Wi...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1   @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3   @sxsw I hope this year's festival isn't as cra...   \n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "7   #SXSW is just starting, #CTIA is around the co...   \n",
       "8   Beautifully smart and simple idea RT @madebyma...   \n",
       "9   Counting down the days to #sxsw plus strong Ca...   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...   \n",
       "11  Find &amp; Start Impromptu Parties at #SXSW Wi...   \n",
       "\n",
       "   emotion_in_tweet_is_directed_at  \\\n",
       "0                           iPhone   \n",
       "1               iPad or iPhone App   \n",
       "2                             iPad   \n",
       "3               iPad or iPhone App   \n",
       "4                           Google   \n",
       "7                          Android   \n",
       "8               iPad or iPhone App   \n",
       "9                            Apple   \n",
       "10                         Android   \n",
       "11                     Android App   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product text  \n",
       "0                                    Negative emotion       \n",
       "1                                    Positive emotion       \n",
       "2                                    Positive emotion       \n",
       "3                                    Negative emotion       \n",
       "4                                    Positive emotion       \n",
       "7                                    Positive emotion       \n",
       "8                                    Positive emotion       \n",
       "9                                    Positive emotion       \n",
       "10                                   Positive emotion       \n",
       "11                                   Positive emotion       "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                            object\n",
       "emotion_in_tweet_is_directed_at                       object\n",
       "is_there_an_emotion_directed_at_a_brand_or_product    object\n",
       "text                                                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.is_there_an_emotion_directed_at_a_brand_or_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 4)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlu-reIH0Na"
   },
   "outputs": [],
   "source": [
    "tweet_data=tweet_data[(tweet_data.is_there_an_emotion_directed_at_a_brand_or_product == 'Positive emotion') | (tweet_data.is_there_an_emotion_directed_at_a_brand_or_product == 'Negative emotion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 4)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product text  \n",
       "0                                   Negative emotion       \n",
       "1                                   Positive emotion       \n",
       "2                                   Positive emotion       \n",
       "3                                   Negative emotion       \n",
       "4                                   Positive emotion       "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n",
    "#### Use `vect` as the variable name for initialising CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices \n",
    "vect = CountVectorizer()\n",
    "tweet_data_dtm = vect.fit_transform(tweet_data.tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000  02  03  08  10  100  100s  100tc  101  106  ...    ûïmute  \\\n",
      "0       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "1       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "2       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "4       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "5       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "6       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "7       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "8       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "9       0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "10      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "11      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "12      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "13      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "14      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "15      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "16      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "17      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "18      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "19      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "20      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "21      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "22      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "23      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "24      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "25      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "26      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "27      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "28      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "29      0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "...   ...  ..  ..  ..  ..  ...   ...    ...  ...  ...  ...       ...   \n",
      "3161    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3162    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3163    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3164    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3165    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3166    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3167    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3168    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3169    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3170    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3171    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3172    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3173    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3174    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3175    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3176    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3177    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3178    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3179    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3180    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3181    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3182    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3183    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3184    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3185    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3186    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3187    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3188    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3189    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "3190    0   0   0   0   0    0     0      0    0    0  ...         0   \n",
      "\n",
      "      ûïspecials  ûïthe  ûïview  ûò  ûòand  ûó  ûójust  ûólewis  ûóthe  \n",
      "0              0      0       0   0      0   0       0        0      0  \n",
      "1              0      0       0   0      0   0       0        0      0  \n",
      "2              0      0       0   0      0   0       0        0      0  \n",
      "3              0      0       0   0      0   0       0        0      0  \n",
      "4              0      0       0   0      0   0       0        0      0  \n",
      "5              0      0       0   0      0   0       0        0      0  \n",
      "6              0      0       0   0      0   0       0        0      0  \n",
      "7              0      0       0   0      0   0       0        0      0  \n",
      "8              0      0       0   0      0   0       0        0      0  \n",
      "9              0      0       0   0      0   0       0        0      0  \n",
      "10             0      0       0   0      0   0       0        0      0  \n",
      "11             0      0       0   0      0   0       0        0      0  \n",
      "12             0      0       0   0      0   0       0        0      0  \n",
      "13             0      0       0   0      0   0       0        0      0  \n",
      "14             0      0       0   0      0   0       0        0      0  \n",
      "15             0      0       0   0      0   0       0        0      0  \n",
      "16             0      0       0   0      0   0       0        0      0  \n",
      "17             0      0       0   0      0   0       0        0      0  \n",
      "18             0      0       0   0      0   0       0        0      0  \n",
      "19             0      0       0   0      0   0       0        0      0  \n",
      "20             0      0       0   0      0   0       0        0      0  \n",
      "21             0      0       0   0      0   0       0        0      0  \n",
      "22             0      0       0   0      0   0       0        0      0  \n",
      "23             0      0       0   0      0   0       0        0      0  \n",
      "24             0      0       0   0      0   0       0        0      0  \n",
      "25             0      0       0   0      0   0       0        0      0  \n",
      "26             0      0       0   0      0   0       0        0      0  \n",
      "27             0      0       0   0      0   0       0        0      0  \n",
      "28             0      0       0   0      0   0       0        0      0  \n",
      "29             0      0       0   0      0   0       0        0      0  \n",
      "...          ...    ...     ...  ..    ...  ..     ...      ...    ...  \n",
      "3161           0      0       0   0      0   0       0        0      0  \n",
      "3162           0      0       0   0      0   0       0        0      0  \n",
      "3163           0      0       0   0      0   0       0        0      0  \n",
      "3164           0      0       0   0      0   0       0        0      0  \n",
      "3165           0      0       0   0      0   0       0        0      0  \n",
      "3166           0      0       0   0      0   0       0        0      0  \n",
      "3167           0      0       0   0      0   0       0        0      0  \n",
      "3168           0      0       0   0      0   0       0        0      0  \n",
      "3169           0      0       0   0      0   0       0        0      0  \n",
      "3170           0      0       0   0      0   0       0        0      0  \n",
      "3171           0      0       0   0      0   0       0        0      0  \n",
      "3172           0      0       0   0      0   0       0        0      0  \n",
      "3173           0      0       0   0      0   0       0        0      0  \n",
      "3174           0      0       0   0      0   0       0        0      0  \n",
      "3175           0      0       0   0      0   0       0        0      0  \n",
      "3176           0      0       0   0      0   0       0        0      0  \n",
      "3177           0      0       0   0      0   0       0        0      0  \n",
      "3178           0      0       0   0      0   0       0        0      0  \n",
      "3179           0      0       0   0      0   0       0        0      0  \n",
      "3180           0      0       0   0      0   0       0        0      0  \n",
      "3181           0      0       0   0      0   0       0        0      0  \n",
      "3182           0      0       0   0      0   0       0        0      0  \n",
      "3183           0      0       0   0      0   0       0        0      0  \n",
      "3184           0      0       0   0      0   0       0        0      0  \n",
      "3185           0      0       0   0      0   0       0        0      0  \n",
      "3186           0      0       0   0      0   0       0        0      0  \n",
      "3187           0      0       0   0      0   0       0        0      0  \n",
      "3188           0      0       0   0      0   0       0        0      0  \n",
      "3189           0      0       0   0      0   0       0        0      0  \n",
      "3190           0      0       0   0      0   0       0        0      0  \n",
      "\n",
      "[3191 rows x 5648 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(tweet_data_dtm.toarray(), columns=vect.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191, 5648)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4LUM-XPH0Nn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zazzle', 'zazzlesxsw', 'zazzlsxsw', 'ze', 'zelda', 'zeldman', 'zero', 'zimride', 'zing', 'zip', 'zite', 'zms', 'zombies', 'zomg', 'zone', 'zoom', 'zzzs', '¼¼', 'á¾_î¾ð', 'äá', 'å_', 'åç', 'åçwhat', 'çü', 'èï', 'ðü', 'öý', 'ù_¾', 'û_', 'ûª', 'ûªll', 'ûªm', 'ûªs', 'ûªt', 'ûï', 'ûï35', 'ûïbuttons', 'ûïfoursquare', 'ûïline', 'ûïmore', 'ûïmute', 'ûïspecials', 'ûïthe', 'ûïview', 'ûò', 'ûòand', 'ûó', 'ûójust', 'ûólewis', 'ûóthe']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print (vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 17. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1DQ2LdNH0Nu"
   },
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(vect.get_feature_names(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwtgjTBeH0Ny"
   },
   "source": [
    "#### Tip: To see all available functions for an Object use dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2n_iCcTNH0N0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648\n"
     ]
    }
   ],
   "source": [
    "print(len(counts_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 18. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive emotion    2672\n",
      "Negative emotion     519\n",
      "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product text  label  \n",
       "0                                   Negative emotion           0  \n",
       "1                                   Positive emotion           1  \n",
       "2                                   Positive emotion           1  \n",
       "3                                   Negative emotion           0  \n",
       "4                                   Positive emotion           1  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data_encode = tweet_data.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "tweet_data_encode['label'] = lb_make.fit_transform(tweet_data['is_there_an_emotion_directed_at_a_brand_or_product'])\n",
    "\n",
    "tweet_data_encode.head() #Results in appending a new column to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "X = tweet_data_encode['tweet_text']\n",
    "Y= tweet_data_encode['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8347    tried installing @mention on my iphone but it ...\n",
       "2381    #iPad2 rocks #SXSW (@mention Apple POP UP Stor...\n",
       "8703    What's your take on iPad? @mention I really wa...\n",
       "4152    : Aron Pilhofer from The New York Times just e...\n",
       "3368    &lt;---- Guess who won an iPad at the #unsix t...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8135    Apple #sxsw pop-store has iPads again. 16gb wi...\n",
       "367     ÛÏ@mention Best thing I've heard this weekend...\n",
       "4721    Anybody know whether I can nab white, 3G, 64GB...\n",
       "7072    Apple to open iPad2 popup shop @mention core o...\n",
       "7047    So many Google products. isn't it time to  tra...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8347    0\n",
       "2381    1\n",
       "8703    1\n",
       "4152    1\n",
       "3368    1\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8135    1\n",
       "367     0\n",
       "4721    1\n",
       "7072    1\n",
       "7047    1\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 21. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8558897243107769\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "#vect = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583959899749374\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  24855\n",
      "Accuracy:  0.8558897243107769\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  4681\n",
      "Accuracy:  0.8533834586466166\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fUhff-oH0O8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.8107769423558897\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english',max_features=300)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15000\n",
      "Accuracy:  0.8533834586466166\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2),max_features=15000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7764\n",
      "Accuracy:  0.8583959899749374\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect = CountVectorizer(ngram_range=(1, 2),min_df=2)\n",
    "tokenize_test(vect)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R8_Internal_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
