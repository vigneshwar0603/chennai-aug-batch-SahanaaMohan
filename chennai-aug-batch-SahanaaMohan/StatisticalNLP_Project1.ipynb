{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Rc_ahEnTF9m"
   },
   "source": [
    "# Predict tags on StackOverflow with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5hmogPBTF9p"
   },
   "source": [
    "In this assignment you will learn how to predict tags for posts from [StackOverflow](https://stackoverflow.com). To solve this task you will use multilabel classification approach.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "In this task you will need the following libraries:\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [Pandas](https://pandas.pydata.org) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSq4Uzh6TF9q"
   },
   "source": [
    "### Data\n",
    "\n",
    "You can find all data required for this assignment into the folder `/data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V1gba1KTF9r"
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnvsWcP9TF9t"
   },
   "source": [
    "For this assignment you will need to use a list of stop words. It can be downloaded from *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "R-RxVRvsTF9u",
    "outputId": "bb9bb432-14c0-4fb5-878c-e02bd816423e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sahanaa_M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sahanaa_M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErHWjimITF91"
   },
   "source": [
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ufeO-nfVGha"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDahiblaU-e7"
   },
   "source": [
    "Literal_eval package takes care of the preprocessing for the string so that it can be used in python. To know more on literal_eval please see the below documentation <br>\n",
    "https://kite.com/python/docs/ast.literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bubX7TSLVLXR"
   },
   "source": [
    "## Task 1: Create training, testing and validation data from the files given. Use title to be the independent variable and tags to be the dependent variable ( 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBFMf3bhVPb_"
   },
   "source": [
    "Note: Ensure you apply literal_eval function on the tags column to ensure all the tags are readable in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8iH7o77TF96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [r]\n",
       "1            [php, mysql]\n",
       "2                    [c#]\n",
       "3    [javascript, jquery]\n",
       "4                  [java]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_table('NLPFiles/train.tsv')\n",
    "x_train = x['title']\n",
    "y_train = x['tags']\n",
    "y_train = y_train.map(lambda tag:literal_eval(tag))\n",
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPaUfSVvTF99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [php, sql]\n",
       "1                   [javascript]\n",
       "2          [ruby-on-rails, ruby]\n",
       "3                [ruby, session]\n",
       "4    [ruby-on-rails, ruby, json]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_table('NLPFiles/validation.tsv')\n",
    "x_val = x['title']\n",
    "y_val = x['tags']\n",
    "y_val = y_val.map(lambda tag:literal_eval(tag))\n",
    "y_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Warning: mysql_query() expects parameter 2 to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get click coordinates from &lt;input type='image'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to implement cloud storage for media asset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is catcomplete in jQuery's autocomplete p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Error building Android app with Cordova 3.1 CLI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Warning: mysql_query() expects parameter 2 to ...\n",
       "1  get click coordinates from <input type='image'...\n",
       "2  How to implement cloud storage for media asset...\n",
       "3  What is catcomplete in jQuery's autocomplete p...\n",
       "4    Error building Android app with Cordova 3.1 CLI"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.read_table('NLPFiles/test.tsv')\n",
    "x_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bcb3kXUSTF-M"
   },
   "source": [
    "## Task 2 (Pre-processing). Implement the function *text_prepare* following the instructions. After that, run the function *test_test_prepare* to test it on tiny cases. (10 points)\n",
    "\n",
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5*, *?*,  *{}*, etc. To prevent the problems, it's usually useful to prepare the data in a custom way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBSchmxtTF-N"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iI1plv6WTF-R"
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def text_prepare(text): \n",
    "    text = text.lower() ### text to lowercase\n",
    "    text = REPLACE_BY_SPACE_RE.sub(r' ',text)\n",
    "    text =  BAD_SYMBOLS_RE.sub(r'',text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    text = ' '.join([w for w in text_tokens if not w in STOPWORDS])             \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5LQEgMATF-Y"
   },
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t2DpralVokG"
   },
   "source": [
    "Execute the test_text_prepare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihi8I2yQTF-b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basic tests are passed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ettHAZ8eVt1R"
   },
   "source": [
    "*Note: You should pass the above test to ensure the text preprocessing is done before our analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QV5JC6HXTF-k"
   },
   "source": [
    "Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOZKuHnGTF-k"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.map(lambda x : text_prepare(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfWls32MV5Uu"
   },
   "source": [
    "Print the top 5 elements in x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BP3LiGsTF-o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               draw stacked dotplot r\n",
       "1    mysql select records datetime field less speci...\n",
       "2                       terminate windows phone 81 app\n",
       "3         get current time specific country via jquery\n",
       "4                           configuring tomcat use ssl\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "At_ZTeqTTF-t"
   },
   "source": [
    "## Task 2 (WordsTagsCount) - Find 3 most popular tags and 3 most popular words in the train data. - 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2mT1f7gWKSu"
   },
   "source": [
    "Note: The words which appear the most are considered as popular in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-eQoxipTF-v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: 'words' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8278</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5800</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5614</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5501</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5055</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words_cnt  words\n",
       "0       8278  using\n",
       "1       5800      c\n",
       "2       5614    php\n",
       "3       5501   java\n",
       "4       5055   file"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    words += x_train[i].split()\n",
    "\n",
    "df_words = pd.DataFrame(words, columns=['words'])\n",
    "df_words = df_words.groupby(by='words').agg({'words':'count'})\n",
    "df_words = df_words.sort_values(by=['words'],ascending=False)\n",
    "df_words['words_cnt'] =  df_words['words']\n",
    "df_words.drop(['words'], axis=1, inplace=True)\n",
    "df_words['words'] = df_words.index\n",
    "df_words.reset_index(drop=True, inplace=True)\n",
    "df_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkwg8_WNTF-3"
   },
   "source": [
    "We are assuming that *tags_counts* and *words_counts* are dictionaries like `{'some_word_or_tag': frequency}`. After applying the sorting procedure, results will be look like this: `[('most_popular_word_or_tag', frequency), ('less_popular_word_or_tag', frequency), ...]`.\n",
    "\n",
    "eg: \n",
    "Tag 1 - 100 Tag 2 - 65 Tag 3 - 250 <br>\n",
    "after sorting looks like, <br>\n",
    "Tag 3 - 250 Tag 1 - 100 Tag 2 - 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSCPa54uTF-4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: 'tags' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_cnt</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19078</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19077</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18661</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13907</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8940</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tags_cnt        tags\n",
       "0     19078  javascript\n",
       "1     19077          c#\n",
       "2     18661        java\n",
       "3     13907         php\n",
       "4      8940      python"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    tags += y_train[i]\n",
    "\n",
    "df_tags = pd.DataFrame(tags, columns=['tags'])\n",
    "df_tags = df_tags.groupby(by='tags').agg({'tags':'count'})\n",
    "df_tags = df_tags.sort_values(by=['tags'],ascending=False)\n",
    "df_tags['tags_cnt'] =  df_tags['tags']\n",
    "df_tags.drop(['tags'], axis=1, inplace=True)\n",
    "df_tags['tags'] = df_tags.index\n",
    "df_tags.reset_index(drop=True, inplace=True)\n",
    "df_tags.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk7M4uJ_TF-8"
   },
   "source": [
    "## Task - 3 Transforming text to a vector (10 points)\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ER1oq48TF-9"
   },
   "outputs": [],
   "source": [
    "#ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    words = text.split()\n",
    "    result_vector = [0 for x in range(dict_size)]\n",
    "    for w in words:\n",
    "        if w in words_to_index:\n",
    "            index = words_to_index[w]\n",
    "            result_vector[index] += 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnLYWnsaTF_A"
   },
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans):\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOlSiUqEW3jD"
   },
   "source": [
    "Execute the test_text_prepare function <br>\n",
    "*<u>Note:</u> You should pass the above test to ensure BOW is working correctly!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqWhoF24TF_D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basic tests are passed.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_my_bag_of_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXW3ALCITF_H"
   },
   "source": [
    "Now apply the implemented function to all samples (this might take up to a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m39xHB2yTF_H"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "pop_words = df_words.words[0:DICT_SIZE]\n",
    "cnt = 0\n",
    "WORDS_TO_INDEX = {}\n",
    "for i in range(DICT_SIZE):\n",
    "    WORDS_TO_INDEX[pop_words[cnt]] = cnt\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DZwPWB1TF_K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (1, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPXTUeNSTF_N"
   },
   "source": [
    "As you might notice, we transform the data to sparse representation, to store the useful information efficiently. There are many types: of such representations, however sklearn algorithms can work only with  csr matrix, so we will use this one.<br>\n",
    "<u>Documentations on sparse matrix:</u> <br>\n",
    "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) \n",
    "(https://docs.scipy.org/doc/scipy/reference/sparse.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8AMzsVxTF_Q"
   },
   "source": [
    "\n",
    "For the 11th row in *X_train_mybag* find how many non-zero elements it has. In this task the answer (variable *non_zero_elements_count*) should be a number, e.g. 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOetzSLzTF_S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero elements in 11th row: 6\n"
     ]
    }
   ],
   "source": [
    "no_of_nonzero_el = 0\n",
    "for i in range(5000):\n",
    "    if(X_train_mybag[11,i] > 0):\n",
    "        no_of_nonzero_el +=1\n",
    "print('Number of non-zero elements in 11th row:',no_of_nonzero_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYAFKGfKTF_V"
   },
   "source": [
    "## Task 4 - TF-IDF (5 points)\n",
    "\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "\n",
    "Implement function *tfidf_features* using class  from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1s3uEKmjYpuh"
   },
   "source": [
    "## Write a function which takes x_train, x_val and x_test as input and return the tf-idf features of the same and the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkz1RYsBTF_Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6V7FPirTF_i"
   },
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        #min_df = 5 -> ignore words that occur in less than 5 titles\n",
    "        min_df=5,\n",
    "        #max_df = 0.90 -> ignore words that occur more than in 90% titles\n",
    "        max_df=0.90)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySnV2zCBTF_p"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(x_train, x_val, x_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_aopTTNL5G1"
   },
   "source": [
    "Print the index of string \"C#\" in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ-3wqTTMCRi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C# is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "if('C#' in tfidf_vocab):\n",
    "    tfidf_vocab['C#']\n",
    "else:\n",
    "    print('C# is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYYO2kdiTF_6"
   },
   "source": [
    "## Task 5: Classification (15 points)\n",
    "MultiLabel classifier\n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use MultiLabelBinarizer from sklearn. <br>\n",
    "<u>Documentation:</u> <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojeCe_e_TF_7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8JYQr8YTF_-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:930: UserWarning: unknown class(es) ['.net', 'ajax', 'algorithm', 'android', 'angularjs', 'apache', 'arrays', 'asp.net', 'asp.net-mvc', 'c', 'c#', 'c++', 'class', 'cocoa-touch', 'codeigniter', 'css', 'csv', 'database', 'date', 'datetime', 'django', 'dom', 'eclipse', 'entity-framework', 'excel', 'facebook', 'file', 'forms', 'function', 'generics', 'google-maps', 'hibernate', 'html', 'html5', 'image', 'ios', 'iphone', 'java', 'javascript', 'jquery', 'json', 'jsp', 'laravel', 'linq', 'linux', 'list', 'loops', 'maven', 'mongodb', 'multithreading', 'mysql', 'node.js', 'numpy', 'objective-c', 'oop', 'opencv', 'osx', 'pandas', 'parsing', 'performance', 'php', 'pointers', 'python', 'python-2.7', 'python-3.x', 'qt', 'r', 'regex', 'rest', 'ruby', 'ruby-on-rails', 'ruby-on-rails-3', 'selenium', 'servlets', 'session', 'sockets', 'sorting', 'spring', 'spring-mvc', 'sql', 'sql-server', 'string', 'swift', 'swing', 'twitter-bootstrap', 'uitableview', 'unit-testing', 'validation', 'vb.net', 'visual-studio', 'visual-studio-2010', 'wcf', 'web-services', 'windows', 'winforms', 'wordpress', 'wpf', 'xaml', 'xcode', 'xml'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(df_tags.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qN1NUA3DTGAB"
   },
   "source": [
    "In this task we suggest to use One-vs-Rest approach, which is implemented in [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) class. In this approach *k* classifiers (= number of tags) are trained. As a basic classifier, use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large.\n",
    "\n",
    "**OneVsRest multi-label strategy**\n",
    "\n",
    "The Multi-label algorithm accepts a binary mask over multiple labels. The result for each prediction will be an array of 0s and 1s marking which class labels apply to each row input sample.\n",
    "\n",
    "**Logistic Regression & SVM**\n",
    "\n",
    "OneVsRest strategy can be used for multi-label learning, where a classifier is used to predict multiple labels for instance. LR & SVM supports multi-class, but we are in a multi-label scenario, therefore, we wrap classifiers in the OneVsRestClassifier.\n",
    "\n",
    "*If you want to learn more about OneVsRest, check out these links:*\n",
    "- *https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5*\n",
    "- *https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff*\n",
    "- *https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Pj6MDkbTGAC"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUHZR2TWTGAF"
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    clf = OneVsRestClassifier(LinearSVC())\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf  ### clf is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k660NfRFTGAL"
   },
   "source": [
    "Train the classifiers for different data transformations: *bag-of-words* and *tf-idf*.\n",
    "classifier_mybag = model for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXyhN-yDTGAP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 0 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 1 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 0 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 1 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    }
   ],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijQZCd9cTGAU"
   },
   "source": [
    "Now you can create predictions for the data. You will need two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QT7uWddTGAV"
   },
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9n0TEtHTGAj"
   },
   "source": [
    "Now, we would need to compare the results of different predictions, e.g. to see whether TF-IDF transformation helps or to try different regularization techniques in logistic regression. For all these experiments, we need to setup evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVqAxDxqTGAk"
   },
   "source": [
    "## Evaluation (10 points)\n",
    "\n",
    "To evaluate the results we will use several classification metrics:\n",
    " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    " - [Area under ROC-curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " - [Area under precision-recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    " \n",
    "Make sure you are familiar with all of them. If you want a refresher, you can click the link to their documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YmNhVsU9K9-"
   },
   "source": [
    "## Import the necessary libraries for the above metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBvrkOUYTGAl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNkYBl_wTGAp"
   },
   "source": [
    "Define the function *print_evaluation_scores* which takes y_val and predicted as input calculates and prints the following output:\n",
    " - *accuracy*\n",
    " - *F1-score - Average = 'weighted'* \n",
    " - *Precision - Average = 'macro'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4nevWnhTGAq"
   },
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print('Accuracy', accuracy_score(y_val, predicted))\n",
    "    print('F1 score', f1_score(y_val, predicted, average='weighted'))\n",
    "    print('Precision', precision_score(y_val, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G92GIO4dTGAt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "Accuracy 1.0\n",
      "F1 score 0\n",
      "Precision 0.0\n",
      "Tfidf\n",
      "Accuracy 1.0\n",
      "F1 score 0\n",
      "Precision 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kT7oso8ZTGAv"
   },
   "source": [
    "You might also want to plot some form of the [ROC curve](http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) for the case of multi-label classification. The input parameters for the roc curve are:\n",
    " - true labels\n",
    " - decision functions scores\n",
    " - number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4hbW3SeBVxz"
   },
   "source": [
    "Import the roc_auc function from the metrics.py file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_9M0AI4TGAv"
   },
   "outputs": [],
   "source": [
    "%run metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_predicted_labels_mybag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZaymN4uTGA0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:651: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5d877236e550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mroc_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_predicted_labels_mybag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_predicted_scores_mybag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\GreatLakes\\Stalin\\chennai-aug-batch-ssbruno-master\\chennai-aug-batch-ssbruno-master\\Residency8\\metrics.py\u001b[0m in \u001b[0;36mroc_auc\u001b[1;34m(y_test, y_score, n_classes)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'aqua'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'darkorange'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cornflowerblue'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         plt.plot(fpr[i], tpr[i], color=color, lw=2, \n\u001b[0m\u001b[0;32m     45\u001b[0m                  label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_auc(y_val_predicted_labels_mybag, y_val_predicted_scores_mybag, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S3Ddz7LTGA3"
   },
   "source": [
    "## Task 4 (MultilabelClassification) - Optional \n",
    "** Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use *F1-score weighted* as an evaluation metric. Our recommendation:\n",
    "- compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n",
    "- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ers40KuOTGA4"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdqTO269TGA8"
   },
   "source": [
    "When you are happy with the quality, create predictions for *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M51F10f9TGBB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stats_NLP_Project_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
