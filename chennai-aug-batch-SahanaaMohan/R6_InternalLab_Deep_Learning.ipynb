{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnSsH8sNOB6F",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Reset Default graph - Needed only for Jupyter notebook\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       object\n",
       "symbol     object\n",
       "open      float64\n",
       "close     float64\n",
       "low       float64\n",
       "high      float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data=data.drop(['date','symbol'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "mini_data=data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "X = mini_data[['open','close','low','high']]\n",
    "y = mini_data['volume']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the graph in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xK0zBd1VOB64",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define input data placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDrYlWTuOB66"
   },
   "outputs": [],
   "source": [
    "#Input features\n",
    "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
    "\n",
    "x_n = tf.layers.batch_normalization(x, training=True)\n",
    "\n",
    "#Actual Prices\n",
    "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
    "b = tf.Variable(tf.zeros(shape=[1]), name=\"Bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "y = tf.add(tf.matmul(x_n,W),b,name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execute the Graph for 100 epochs and observe the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f"
   },
   "outputs": [],
   "source": [
    "#Lets start graph Execution\n",
    "sess = tf.Session()\n",
    "\n",
    "# variables need to be initialized before we can use them\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#how many times data need to be shown to model\n",
    "training_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9smwOW-1OB7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step:  0  is  223195040000000.0\n",
      "Training loss at step:  10  is  nan\n",
      "Training loss at step:  20  is  nan\n",
      "Training loss at step:  30  is  nan\n",
      "Training loss at step:  40  is  nan\n",
      "Training loss at step:  50  is  nan\n",
      "Training loss at step:  60  is  nan\n",
      "Training loss at step:  70  is  nan\n",
      "Training loss at step:  80  is  nan\n",
      "Training loss at step:  90  is  nan\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "            \n",
    "    #Calculate train_op and loss\n",
    "    _, train_loss = sess.run([train_op, loss],feed_dict={x:X_train, \n",
    "                                                        y_:y_train})\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JuLI6bSOB7n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b\n",
    "\n",
    "Hint: Use sess.run(W) to get W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.shape(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.shape(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZqKUEFsOB71"
   },
   "source": [
    "### Find the Absolute mean square loss difference between training and testing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZUAjZ5oOB78"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "### Linear Classification using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GoNTWXAOB8C",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
    "#### Use Mean square error as loss function and sgd as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpeL5rCTOB8D"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Normalize input data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "#Compile the model - add Loss and Gradient Descent optimizer\n",
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt-HYFMEOB8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66JGJt7GOB8H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "700/700 [==============================] - 0s 450us/step - loss: nan          \n",
      "Epoch 2/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 3/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 4/100\n",
      "700/700 [==============================] - 0s 44us/step - loss: nan\n",
      "Epoch 5/100\n",
      "700/700 [==============================] - 0s 43us/step - loss: nan\n",
      "Epoch 6/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 7/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 8/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 9/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 10/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: nan\n",
      "Epoch 11/100\n",
      "700/700 [==============================] - 0s 44us/step - loss: nan\n",
      "Epoch 12/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 13/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 14/100\n",
      "700/700 [==============================] - 0s 60us/step - loss: nan\n",
      "Epoch 15/100\n",
      "700/700 [==============================] - 0s 61us/step - loss: nan\n",
      "Epoch 16/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 17/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 18/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 19/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 20/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 21/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 22/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 23/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 24/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 25/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 26/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 27/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 28/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 29/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 30/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 31/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 32/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 33/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 34/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 35/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 36/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 37/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 38/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 39/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 40/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 41/100\n",
      "700/700 [==============================] - 0s 57us/step - loss: nan\n",
      "Epoch 42/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 43/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: nan\n",
      "Epoch 44/100\n",
      "700/700 [==============================] - 0s 44us/step - loss: nan\n",
      "Epoch 45/100\n",
      "700/700 [==============================] - 0s 44us/step - loss: nan\n",
      "Epoch 46/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 47/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 48/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 49/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 50/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: nan\n",
      "Epoch 51/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 52/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 53/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 54/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 55/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 56/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 57/100\n",
      "700/700 [==============================] - 0s 63us/step - loss: nan\n",
      "Epoch 58/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 59/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 60/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 61/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 62/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 63/100\n",
      "700/700 [==============================] - 0s 57us/step - loss: nan\n",
      "Epoch 64/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 65/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 66/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 67/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 68/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 69/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 70/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 71/100\n",
      "700/700 [==============================] - 0s 57us/step - loss: nan\n",
      "Epoch 72/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 73/100\n",
      "700/700 [==============================] - 0s 59us/step - loss: nan\n",
      "Epoch 74/100\n",
      "700/700 [==============================] - 0s 61us/step - loss: nan\n",
      "Epoch 75/100\n",
      "700/700 [==============================] - 0s 63us/step - loss: nan\n",
      "Epoch 76/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 77/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 78/100\n",
      "700/700 [==============================] - 0s 67us/step - loss: nan\n",
      "Epoch 79/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 80/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 81/100\n",
      "700/700 [==============================] - 0s 57us/step - loss: nan\n",
      "Epoch 82/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 83/100\n",
      "700/700 [==============================] - 0s 56us/step - loss: nan\n",
      "Epoch 84/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 85/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 86/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: nan\n",
      "Epoch 87/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 88/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 89/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: nan\n",
      "Epoch 90/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: nan\n",
      "Epoch 91/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 92/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 93/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 94/100\n",
      "700/700 [==============================] - 0s 61us/step - loss: nan\n",
      "Epoch 95/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 96/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: nan\n",
      "Epoch 97/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: nan\n",
      "Epoch 98/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n",
      "Epoch 99/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: nan\n",
      "Epoch 100/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23e0ce45fd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "y = data[['Species']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahanaa_M\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_sklearn = y.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "y_sklearn = lb_make.fit_transform(y)\n",
    "\n",
    "#y_sklearn.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(y_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2\n",
       "0    1  0  0\n",
       "1    1  0  0\n",
       "2    1  0  0\n",
       "3    1  0  0\n",
       "4    1  0  0\n",
       "5    1  0  0\n",
       "6    1  0  0\n",
       "7    1  0  0\n",
       "8    1  0  0\n",
       "9    1  0  0\n",
       "10   1  0  0\n",
       "11   1  0  0\n",
       "12   1  0  0\n",
       "13   1  0  0\n",
       "14   1  0  0\n",
       "15   1  0  0\n",
       "16   1  0  0\n",
       "17   1  0  0\n",
       "18   1  0  0\n",
       "19   1  0  0\n",
       "20   1  0  0\n",
       "21   1  0  0\n",
       "22   1  0  0\n",
       "23   1  0  0\n",
       "24   1  0  0\n",
       "25   1  0  0\n",
       "26   1  0  0\n",
       "27   1  0  0\n",
       "28   1  0  0\n",
       "29   1  0  0\n",
       "..  .. .. ..\n",
       "120  0  0  1\n",
       "121  0  0  1\n",
       "122  0  0  1\n",
       "123  0  0  1\n",
       "124  0  0  1\n",
       "125  0  0  1\n",
       "126  0  0  1\n",
       "127  0  0  1\n",
       "128  0  0  1\n",
       "129  0  0  1\n",
       "130  0  0  1\n",
       "131  0  0  1\n",
       "132  0  0  1\n",
       "133  0  0  1\n",
       "134  0  0  1\n",
       "135  0  0  1\n",
       "136  0  0  1\n",
       "137  0  0  1\n",
       "138  0  0  1\n",
       "139  0  0  1\n",
       "140  0  0  1\n",
       "141  0  0  1\n",
       "142  0  0  1\n",
       "143  0  0  1\n",
       "144  0  0  1\n",
       "145  0  0  1\n",
       "146  0  0  1\n",
       "147  0  0  1\n",
       "148  0  0  1\n",
       "149  0  0  1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_sklearn, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Build the model with following layers: <br>\n",
    "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
    "2. Second Dense layer with 8 neurons <br>\n",
    "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
    "4. Use SGD and categorical_crossentropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Normalize input data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
    "\n",
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-8086695f2afb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_sklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m105\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_sklearn.reshape(105), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Accuracy of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Linear+classification+using+Tensorflow+and+Keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
